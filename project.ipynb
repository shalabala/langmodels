{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from books_dataset import BooksDataset\n",
    "from utility.training import TrainingLoop\n",
    "from utility.trainsettings import TrainSettings\n",
    "from utility.evaluator import AccuracyEvaluator\n",
    "from books_model import BooksModel\n",
    "\n",
    "import constants\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset_tr \u001b[38;5;241m=\u001b[39m \u001b[43mBooksDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/tokens.bin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/vocab.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 90%\u001b[39;00m\n\u001b[1;32m      2\u001b[0m ds_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset_tr)\n",
      "File \u001b[0;32m/media/filestorage2/code/projects/llm/books_dataset.py:30\u001b[0m, in \u001b[0;36mBooksDataset.from_file\u001b[0;34m(sequence_len, tokens_bin_file, vocab_file)\u001b[0m\n\u001b[1;32m     27\u001b[0m shorts_to_unpack \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(batch, count \u001b[38;5;241m-\u001b[39m i)\n\u001b[1;32m     28\u001b[0m tokens_py \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshorts_to_unpack\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m\"\u001b[39m, tf\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mshorts_to_unpack))\n\u001b[0;32m---> 30\u001b[0m tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((tokens,  \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens_py\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m tokens_py\n\u001b[1;32m     33\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m shorts_to_unpack\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_tr = BooksDataset.from_file(32, 'data/tokens.bin', \"data/vocab.txt\") # 80%\n",
    "ds_len = len(dataset_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tr.unsplit()\n",
    "dataset_val = dataset_tr.split_off(int(ds_len * 0.2)) # 10%\n",
    "dataset_test = dataset_val.split_off(int(ds_len * 0.1)) # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "910842096\n",
      "728673677 91084210 91084209\n",
      "['usually', ',', 'he', 'would', 'be', 'tearing', 'around', 'the', 'living', 'room', ',', 'playing', 'with', 'his', 'toys', 'but', 'just', 'one', 'look', 'at', 'a', 'minion', 'sent', 'him', 'practically', 'catatonic', 'that', 'had', 'been', 'megan', \"'s\", 'plan']\n",
      "[',', 'he', 'would', 'be', 'tearing', 'around', 'the', 'living', 'room', ',', 'playing', 'with', 'his', 'toys', 'but', 'just', 'one', 'look', 'at', 'a', 'minion', 'sent', 'him', 'practically', 'catatonic', 'that', 'had', 'been', 'megan', \"'s\", 'plan', 'when']\n",
      "['like', '.', 'i', 'narrowed', 'my', 'eyes', ',', 'smirking', '``', 'you', 'play', 'some', 'rock', 'club', ',', 'and', 'there', \"'s\", 'some', 'young', 'thing', 'at', 'the', 'front', ',', 'all', 'innocent', 'and', 'big', 'eyes', ',', 'and']\n",
      "['.', 'i', 'narrowed', 'my', 'eyes', ',', 'smirking', '``', 'you', 'play', 'some', 'rock', 'club', ',', 'and', 'there', \"'s\", 'some', 'young', 'thing', 'at', 'the', 'front', ',', 'all', 'innocent', 'and', 'big', 'eyes', ',', 'and', 'you']\n",
      "['is', 'he', 'still', 'here', 'jack', 'dammit', '.', 'i', 'pushed', 'at', 'him', 'as', 'he', 'settled', 'into', 'the', 'pillows', ',', 'his', 'breaths', 'getting', 'deeper', '``', 'wake', 'up', ',', 'jack', '.', 'i', 'nudged', 'him', 'again']\n",
      "['he', 'still', 'here', 'jack', 'dammit', '.', 'i', 'pushed', 'at', 'him', 'as', 'he', 'settled', 'into', 'the', 'pillows', ',', 'his', 'breaths', 'getting', 'deeper', '``', 'wake', 'up', ',', 'jack', '.', 'i', 'nudged', 'him', 'again', 'he']\n"
     ]
    }
   ],
   "source": [
    "def dataset_peak(dataset):\n",
    "    item = dataset[0]\n",
    "    print([dataset.vocab[i.item()] for i in item[0]])\n",
    "    print([dataset.vocab[i.item()] for i in item[1]])\n",
    "\n",
    "print(ds_len)\n",
    "print(len(dataset_tr), len(dataset_val), len(dataset_test))\n",
    " \n",
    "dataset_peak(dataset_tr)\n",
    "dataset_peak(dataset_val)\n",
    "dataset_peak(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BooksModel(len(dataset_tr.vocab), 256)\n",
    "settings = TrainSettings(\n",
    "    \"llm\",\n",
    "    model,\n",
    "    dataset_tr,\n",
    "    dataset_val,\n",
    "    epochs=30,\n",
    "    device='cpu', # 'cuda',\n",
    "    batch_size=4, # 128,\n",
    "    lr=0.001,\n",
    "    save_path=\"models/\",\n",
    "    save_after_epoch=5,\n",
    "    print_after_steps=1\n",
    ")\n",
    "\n",
    "training = TrainingLoop(settings, \n",
    "                       \"models/books_v1_training.json\",\n",
    "                        [AccuracyEvaluator()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training llm for 30 epochs\n",
      "Training on cpu\n"
     ]
    }
   ],
   "source": [
    "training.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
